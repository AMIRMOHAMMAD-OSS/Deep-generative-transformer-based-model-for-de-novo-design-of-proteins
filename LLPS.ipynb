{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AMIRMOHAMMAD-OSS/Deep-generative-transformer-based-model-for-de-novo-design-of-proteins/blob/main/LLPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyzTHwf8dRBy",
        "outputId": "19759785-a127-4700-d15f-330b55a5ce1d",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'classi'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 33 (delta 12), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (33/33), 14.07 MiB | 9.17 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n",
            "Collecting slack-sdk\n",
            "  Downloading slack_sdk-3.27.1-py2.py3-none-any.whl (285 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.7/285.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: slack-sdk\n",
            "Successfully installed slack-sdk-3.27.1\n"
          ]
        }
      ],
      "source": [
        "#@title Importing libraries\n",
        "!git clone https://github.com/AMIRMOHAMMAD-OSS/classi\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "!pip install slack-sdk\n",
        "from tqdm.contrib.slack import tqdm, trange\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "from ast import literal_eval\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoTokenizer\n",
        "from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers\n",
        "from transformers import CanineTokenizer, CanineModel\n",
        "tokenizer = Tokenizer.from_file(\"classi/Trained_BPE2.json\")\n",
        "tokenizer.model_max_length = 256\n",
        "tokenizer\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler as Sc\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split as TTS\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split as TTS\n",
        "\n",
        "\n",
        "def edit(x):\n",
        "    for i in x:\n",
        "      if i in \"BJOUX\\Z_n\\n\":\n",
        "        x = x.replace(i,\"\")\n",
        "      else:\n",
        "        pass\n",
        "    return x\n",
        "\n",
        "def encode(x):\n",
        "  l = []\n",
        "  chars = tokenizer.get_vocab()\n",
        "  for i in x:\n",
        "    l.append(chars[i])\n",
        "  return l\n",
        "\n",
        "with open(\"classi/training.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "  k = \"#\".join([i for i in edit(text).split(\"<|edoftext|>\")])\n",
        "chars = tokenizer.get_vocab()\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "\n",
        "with open(\"classi/validation.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "  l = \"#\".join([i for i in edit(text).split(\"<|edoftext|>\")])\n",
        "\n",
        "def Padding(x,PAD = 0,max_len = 512):\n",
        "  l = []\n",
        "  max_len = max(512,len(max(x,key = lambda x: len(x))))\n",
        "  for i in x:\n",
        "    a = i\n",
        "    if len(a) < max_len:\n",
        "      a = a + [PAD for i in range(max_len-len(a))]\n",
        "    l.append(a)\n",
        "  return np.array(l).reshape(len(x),max_len)\n",
        "with open(\"classi/negative_dataset.txt\") as x:\n",
        "  c = x.read()\n",
        "Filtered_train  = [i for i in k.split(\"#\") if len(i)<= 512][1:]\n",
        "Filtered_val = [i for i in l.split(\"#\") if len(i)<= 512][1:]\n",
        "Filtered_neg = [i for i in c.split(\"\\n\") if len(i)<= 512]\n",
        "pos = Filtered_train + Filtered_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "Yc9VXFLYN-x6"
      },
      "outputs": [],
      "source": [
        "#@title Loading models and requierments\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def setup_logging(config):\n",
        "    work_dir = config.system.work_dir\n",
        "    os.makedirs(work_dir, exist_ok=True)\n",
        "    with open(os.path.join(work_dir, 'args.txt'), 'w') as f:\n",
        "        f.write(' '.join(sys.argv))\n",
        "    with open(os.path.join(work_dir, 'config.json'), 'w') as f:\n",
        "        f.write(json.dumps(config.to_dict(), indent=4))\n",
        "\n",
        "class CfgNode:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self._str_helper(0)\n",
        "\n",
        "    def _str_helper(self, indent):\n",
        "        parts = []\n",
        "        for k, v in self.__dict__.items():\n",
        "            if isinstance(v, CfgNode):\n",
        "                parts.append(\"%s:\\n\" % k)\n",
        "                parts.append(v._str_helper(indent + 1))\n",
        "            else:\n",
        "                parts.append(\"%s: %s\\n\" % (k, v))\n",
        "        parts = [' ' * (indent * 4) + p for p in parts]\n",
        "        return \"\".join(parts)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return { k: v.to_dict() if isinstance(v, CfgNode) else v for k, v in self.__dict__.items() }\n",
        "\n",
        "    def merge_from_dict(self, d):\n",
        "        self.__dict__.update(d)\n",
        "\n",
        "    def merge_from_args(self, args):\n",
        "\n",
        "        for arg in args:\n",
        "            keyval = arg.split('=')\n",
        "            assert len(keyval) == 2, \"expecting each override arg to be of form --arg=value, got %s\" % arg\n",
        "            key, val = keyval\n",
        "            try:\n",
        "                val = literal_eval(val)\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "            assert key[:2] == '--'\n",
        "            key = key[2:]\n",
        "            keys = key.split('.')\n",
        "            obj = self\n",
        "            for k in keys[:-1]:\n",
        "                obj = getattr(obj, k)\n",
        "            leaf_key = keys[-1]\n",
        "            assert hasattr(obj, leaf_key), f\"{key} is not an attribute that exists in the config\"\n",
        "            print(\"command line overwriting config attribute %s with %s\" % (key, val))\n",
        "            setattr(obj, leaf_key, val)\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class SiLU(nn.Module):\n",
        "   def forward(self, x):\n",
        "        return x*F.sigmoid(x)\n",
        "\n",
        "class NY(nn.Module):\n",
        "  def forward(self,x):\n",
        "    return 3*torch.tanh(0.3*x)\n",
        "\n",
        "class NewGELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = SelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = nn.ModuleDict(dict(\n",
        "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            act     = NewGELU(),\n",
        "            dropout = nn.Dropout(config.resid_pdrop),\n",
        "        ))\n",
        "        m = self.mlp\n",
        "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x))))\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlpf(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class ClassifierII(nn.Module):\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CfgNode()\n",
        "        C.model_type = 'gpt'\n",
        "        C.n_layer = None\n",
        "        C.n_head = None\n",
        "        C.n_embd =  None\n",
        "        C.vocab_size = len(chars)\n",
        "        C.max_length = 512\n",
        "        C.embd_pdrop = 0.1\n",
        "        C.resid_pdrop = 0.1\n",
        "        C.attn_pdrop = 0.1\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.max_length = config.block_size\n",
        "        self.soft = nn.Softmax(1)\n",
        "        self.config = self.get_default_config()\n",
        "        self.device = \"cuda\"\n",
        "        self.model_states = {'h':{'n_layer': 48, 'n_head': 25, 'n_embd': 1600},\n",
        "                                    'g':{'n_layer': 12, '': 12, 'n_embd': 768},\n",
        "                'f':   {'n_layer': 24, 'n_head': 16, 'n_embd': 1024},\n",
        "                'e':   {'n_layer': 36, 'n_head': 20, 'n_embd': 1280},\n",
        "                'd':{'n_layer': 8, 'n_head': 16, 'n_embd': 512},\n",
        "                'c':{'n_layer': 6, 'n_head': 6, 'n_embd': 192},\n",
        "                'b':{'n_layer': 4, 'n_head': 4, 'n_embd': 128},\n",
        "                'a':{'n_layer': 3, 'n_head': 3, 'n_embd': 48}}\n",
        "\n",
        "        type_ = config.model_type is not None\n",
        "        #assert type_ in \"abcdefgh\"\n",
        "        p = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n",
        "        #assert type_ == True and p == True\n",
        "        if type_:\n",
        "            config.merge_from_dict(self.model_states[config.model_type])\n",
        "        self.closs = nn.BCELoss()\n",
        "        self.ny = NY()\n",
        "        self.l = nn.Linear(512,1,64)\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.max_length, config.n_embd),\n",
        "            drop = nn.Dropout(config.embd_pdrop),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),))\n",
        "        self.classifier_head = nn.Sequential(#nn.Tanh(),\n",
        "                                             nn.Linear(config.n_embd, 2)\n",
        "                                             )\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"[ Number of trainable parameters: %.2fM ]\" % (n_params/1e6,))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        config = cls.get_default_config()\n",
        "        config.model_type = model_type\n",
        "        config.vocab_size = 25\n",
        "        config.max_length = 512\n",
        "        model = ClassifierII(config)\n",
        "        sd = model.state_dict()\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "        keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')]\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        assert len(keys) == len(sd)\n",
        "        for k in keys:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn\n",
        "                if pn.endswith('bias'):\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    no_decay.add(fpn)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    @torch.no_grad\n",
        "    def predict_proba(self,idx,j =\"None\"):\n",
        "      sigmoid = nn.Sigmoid()\n",
        "      soft = nn.Softmax(dim=1)\n",
        "      ny = NY()\n",
        "      si = SiLU()\n",
        "      self.eval()\n",
        "      x,_ = self.forward(idx)\n",
        "      if j == \"None\":\n",
        "        return x[:,0:1]\n",
        "      elif j == \"soft\":\n",
        "        return self.soft(x)[:,0:1]\n",
        "      else:\n",
        "        return sigmoid(x)[:,0:1]\n",
        "\n",
        "    @torch.no_grad\n",
        "    def predict(self,idx):\n",
        "      g = torch.zeros((idx.shape[0],1))\n",
        "      e = -1\n",
        "      for i in self.predict_proba(idx):\n",
        "        e+=1\n",
        "        if i[0].item()>=0.5:\n",
        "          g[e] = 1\n",
        "      return g\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.classifier_head(x).view(b,2,t).to(device)\n",
        "        logits = self.l(logits)\n",
        "        logits = F.sigmoid(self.ny(logits).view(b,2).mean(1).view(b,1))\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.binary_cross_entropy(logits,targets)\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CfgNode()\n",
        "        C.device = 'cuda'\n",
        "        C.num_workers = 4\n",
        "        C.max_iters = None\n",
        "        C.batch_size = 128\n",
        "        C.max_length = 512\n",
        "        C.learning_rate = 8e-4\n",
        "        C.betas = (0.9, 0.95)\n",
        "        C.weight_decay = 0.1\n",
        "        C.grad_norm_clip = 1.0\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config, model, train_dataset,val_dataset):\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.optimizer = None\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.callbacks = defaultdict(list)\n",
        "        self.device ='cuda'\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.iter_num = 0\n",
        "        self.iter_time = 0.0\n",
        "        self.iter_dt = 0.0\n",
        "\n",
        "    def add_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent].append(callback)\n",
        "\n",
        "    def set_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent] = [callback]\n",
        "\n",
        "    def trigger_callbacks(self, onevent: str):\n",
        "        for callback in self.callbacks.get(onevent, []):\n",
        "            callback(self)\n",
        "\n",
        "    def run(self):\n",
        "        model, config = self.model, self.config\n",
        "        self.optimizer = model.configure_optimizers(config)\n",
        "        batch_size = self.config.batch_size\n",
        "        def get_batch(mode):\n",
        "            batch_size = 64\n",
        "            if mode == \"train\":\n",
        "              data = train_dataset\n",
        "              pos_train = torch.tensor(np.array([data[i:i+1,:] for i in range(data.shape[0]) if data[i,-1].item() == 1]).reshape(len([data[i:i+1,:] for i in range(data.shape[0]) if data[i,-1].item() == 1]),513))\n",
        "              neg_train = torch.tensor(np.array([data[i:i+1,:] for i in range(data.shape[0]) if data[i,-1].item() == 0]).reshape(len([data[i:i+1,:] for i in range(data.shape[0]) if data[i,-1].item() == 0]),513))\n",
        "              N = np.random.randint(batch_size)\n",
        "              ix_pos = np.random.randint(pos_train.shape[0]-N)\n",
        "              ix_neg = np.random.randint(neg_train.shape[0]-batch_size-N)\n",
        "              pos_data = pos_train[ix_pos:ix_pos+N,:]\n",
        "              neg_data = neg_train[ix_neg:ix_neg+batch_size-N,:]\n",
        "              data = torch.concat((pos_data,neg_data))\n",
        "              seq = data[:,:-1]\n",
        "              targets = torch.zeros((batch_size,1), device=\"cuda\")\n",
        "              o = -1\n",
        "              for i in data:\n",
        "                o+=1\n",
        "                if i[-1].item() == 1:\n",
        "                  targets[o] = 1\n",
        "                #else:\n",
        "                  #targets[o][1] = 1\n",
        "            else:\n",
        "              data = val_dataset\n",
        "              ix = np.random.randint(data.shape[0]-batch_size)\n",
        "              data = data[ix:ix+batch_size,:]\n",
        "              seq = data[:,:-1]\n",
        "              targets = torch.zeros((batch_size,1), device=\"cuda\")\n",
        "              o = -1\n",
        "              for i in data:\n",
        "                o+=1\n",
        "                if i[-1].item() == 1:\n",
        "                  targets[o] = 1\n",
        "                #else:\n",
        "                 # targets[o][1] = 1\n",
        "            targets = targets.view(batch_size,1).to(\"cuda\")\n",
        "            seq = seq.view(batch_size,512).to(\"cuda\")\n",
        "            return seq, targets\n",
        "\n",
        "        @torch.no_grad\n",
        "        def cross_val():\n",
        "          model.eval()\n",
        "          out = []\n",
        "          for i in [\"train\",\"val\"]:\n",
        "            losses = torch.zeros(200+1)\n",
        "            for k in range(200):\n",
        "              X,Y = get_batch(i)\n",
        "              logits,loss = model(X,Y)\n",
        "              losses[k]=loss.item()\n",
        "              out1 = losses.mean()\n",
        "            out.append(out1)\n",
        "          model.train()\n",
        "          return out\n",
        "        losses = cross_val()\n",
        "        LOSS = [losses]\n",
        "        print(\"\\n[train loss = {k}, val loss =  {j}]\\n\".format(k = losses[0],j = losses[1]))\n",
        "        model.train()\n",
        "        for epoch in range(20):\n",
        "          print(\"[epoch {o}] \\n\".format(o = epoch))\n",
        "          iters = 200\n",
        "          for i in range(iters):\n",
        "            if i == 0:\n",
        "              Y = \"| =\"\n",
        "            elif i == iters -1 :\n",
        "              Y = \"=> 100% |\"\n",
        "            else:\n",
        "              if i%(int(iters/50)) == 0 :\n",
        "                Y = \"=\"\n",
        "              else:\n",
        "                Y = \"\"\n",
        "            print(\"{y}\".format(y = Y),end=\"\")\n",
        "            x, y = get_batch(\"train\")\n",
        "            logits, self.loss = model(x, y)\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            self.loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "            self.optimizer.step()\n",
        "          losses = cross_val()\n",
        "          LOSS.append(losses)\n",
        "          print(LOSS)\n",
        "          print(\"\\n[train loss = {k}, val loss =  {j}]\\n\".format(k = losses[0],j = losses[1]))\n",
        "        PATH = \"model {h}\".format(h = model_config.model_type)\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def setup_logging(config):\n",
        "    work_dir = config.system.work_dir\n",
        "    os.makedirs(work_dir, exist_ok=True)\n",
        "    with open(os.path.join(work_dir, 'args.txt'), 'w') as f:\n",
        "        f.write(' '.join(sys.argv))\n",
        "    with open(os.path.join(work_dir, 'config.json'), 'w') as f:\n",
        "        f.write(json.dumps(config.to_dict(), indent=4))\n",
        "\n",
        "class CfgNode:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self._str_helper(0)\n",
        "\n",
        "    def _str_helper(self, indent):\n",
        "        parts = []\n",
        "        for k, v in self.__dict__.items():\n",
        "            if isinstance(v, CfgNode):\n",
        "                parts.append(\"%s:\\n\" % k)\n",
        "                parts.append(v._str_helper(indent + 1))\n",
        "            else:\n",
        "                parts.append(\"%s: %s\\n\" % (k, v))\n",
        "        parts = [' ' * (indent * 4) + p for p in parts]\n",
        "        return \"\".join(parts)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return { k: v.to_dict() if isinstance(v, CfgNode) else v for k, v in self.__dict__.items() }\n",
        "\n",
        "    def merge_from_dict(self, d):\n",
        "        self.__dict__.update(d)\n",
        "\n",
        "    def merge_from_args(self, args):\n",
        "\n",
        "        for arg in args:\n",
        "            keyval = arg.split('=')\n",
        "            assert len(keyval) == 2, \"expecting each override arg to be of form --arg=value, got %s\" % arg\n",
        "            key, val = keyval\n",
        "            try:\n",
        "                val = literal_eval(val)\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "            assert key[:2] == '--'\n",
        "            key = key[2:]\n",
        "            keys = key.split('.')\n",
        "            obj = self\n",
        "            for k in keys[:-1]:\n",
        "                obj = getattr(obj, k)\n",
        "            leaf_key = keys[-1]\n",
        "            assert hasattr(obj, leaf_key), f\"{key} is not an attribute that exists in the config\"\n",
        "            print(\"command line overwriting config attribute %s with %s\" % (key, val))\n",
        "            setattr(obj, leaf_key, val)\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class SiLU(nn.Module):\n",
        "   def forward(self, x):\n",
        "        return x*F.sigmoid(x)\n",
        "\n",
        "class NY(nn.Module):\n",
        "  def forward(self,x):\n",
        "    return 3*torch.tanh(0.3*x)\n",
        "\n",
        "class NewGELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = SelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = nn.ModuleDict(dict(\n",
        "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            act     = NewGELU(),\n",
        "            dropout = nn.Dropout(config.resid_pdrop),\n",
        "        ))\n",
        "        m = self.mlp\n",
        "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x))))\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlpf(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class ClassifierI(nn.Module):\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CfgNode()\n",
        "        C.model_type = 'gpt'\n",
        "        C.n_layer = None\n",
        "        C.n_head = None\n",
        "        C.n_embd =  None\n",
        "        C.vocab_size = len(chars)\n",
        "        C.max_length = 512\n",
        "        C.embd_pdrop = 0.1\n",
        "        C.resid_pdrop = 0.1\n",
        "        C.attn_pdrop = 0.1\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.max_length = config.block_size\n",
        "        self.soft = nn.Softmax(1)\n",
        "        self.config = self.get_default_config()\n",
        "        self.device = \"cpu\"\n",
        "        self.model_states = {'h':{'n_layer': 48, 'n_head': 25, 'n_embd': 1600},\n",
        "                                    'g':{'n_layer': 12, '': 12, 'n_embd': 768},\n",
        "                'f':   {'n_layer': 24, 'n_head': 16, 'n_embd': 1024},\n",
        "                'e':   {'n_layer': 36, 'n_head': 20, 'n_embd': 1280},\n",
        "                'd':{'n_layer': 8, 'n_head': 16, 'n_embd': 512},\n",
        "                'c':{'n_layer': 6, 'n_head': 6, 'n_embd': 192},\n",
        "                'b':{'n_layer': 4, 'n_head': 4, 'n_embd': 128},\n",
        "                'a':{'n_layer': 3, 'n_head': 3, 'n_embd': 48}}\n",
        "\n",
        "        type_ = config.model_type is not None\n",
        "        #assert type_ in \"abcdefgh\"\n",
        "        p = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n",
        "        #assert type_ == True and p == True\n",
        "        if type_:\n",
        "            config.merge_from_dict(self.model_states[config.model_type])\n",
        "        self.closs = nn.BCELoss()\n",
        "        self.ny = NY()\n",
        "        #self.l = nn.Linear(512,1,64)\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.max_length, config.n_embd),\n",
        "            drop = nn.Dropout(config.embd_pdrop),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),))\n",
        "        self.classifier_head = nn.Sequential(#nn.Tanh(),\n",
        "                                             nn.Linear(config.n_embd, 2)\n",
        "                                             )\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        #print(\"[ Number of trainable parameters: %.2fM ]\" % (n_params/1e6,))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        config = cls.get_default_config()\n",
        "        config.model_type = model_type\n",
        "        config.vocab_size = 25\n",
        "        config.max_length = 512\n",
        "        model = ClassifierI(config)\n",
        "        sd = model.state_dict()\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "        keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')]\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        assert len(keys) == len(sd)\n",
        "        for k in keys:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn\n",
        "                if pn.endswith('bias'):\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    no_decay.add(fpn)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    @torch.no_grad\n",
        "    def predict_proba(self,idx,j =\"None\"):\n",
        "      sigmoid = nn.Sigmoid()\n",
        "      soft = nn.Softmax(dim=1)\n",
        "      ny = NY()\n",
        "      si = SiLU()\n",
        "      self.eval()\n",
        "      x,_ = self.forward(idx)\n",
        "      if j == \"None\":\n",
        "        return x[:,0:1]\n",
        "      elif j == \"soft\":\n",
        "        return self.soft(x)[:,0:1]\n",
        "      else:\n",
        "        return sigmoid(x)[:,0:1]\n",
        "\n",
        "    @torch.no_grad\n",
        "    def predict(self,idx):\n",
        "      g = torch.zeros((idx.shape[0],1))\n",
        "      e = -1\n",
        "      for i in self.predict_proba(idx):\n",
        "        e+=1\n",
        "        if i[0].item()>=0.5:\n",
        "          g[e] = 1\n",
        "      return g\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.ny(self.classifier_head(x)).mean(1).to(device)\n",
        "        #logits = self.l(logits)\n",
        "        # = F.sigmoid(logits.view(b,2).mean(1).view(b,1))\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits,targets,ignore_index = -1)\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CfgNode()\n",
        "        C.device = 'cuda'\n",
        "        C.num_workers = 4\n",
        "        C.max_iters = None\n",
        "        C.batch_size = 64\n",
        "        C.max_length = 512\n",
        "        C.learning_rate = 8e-4\n",
        "        C.betas = (0.9, 0.95)\n",
        "        C.weight_decay = 0.1\n",
        "        C.grad_norm_clip = 1.0\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config, model, train_dataset,val_dataset):\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.optimizer = None\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.callbacks = defaultdict(list)\n",
        "        self.device ='cuda'\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.iter_num = 0\n",
        "        self.iter_time = 0.0\n",
        "        self.iter_dt = 0.0\n",
        "\n",
        "    def add_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent].append(callback)\n",
        "\n",
        "    def set_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent] = [callback]\n",
        "\n",
        "    def trigger_callbacks(self, onevent: str):\n",
        "        for callback in self.callbacks.get(onevent, []):\n",
        "            callback(self)\n",
        "\n",
        "    def run(self):\n",
        "        model, config = self.model, self.config\n",
        "        self.optimizer = model.configure_optimizers(config)\n",
        "        batch_size = self.config.batch_size\n",
        "        def get_batch(mode):\n",
        "            batch_size = 64\n",
        "            if mode == \"train\":\n",
        "              data = train_dataset\n",
        "              pos_train = torch.tensor(np.array([data[i:i+1,:] for i in range(data.shape[0]) if data[i,-1].item() == 1]).reshape(len([data[i:i+1,:] for i in range(data.shape[0]) if data[i,-1].item() == 1]),513))\n",
        "              neg_train = torch.tensor(np.array([data[i:i+1,:] for i in range(data.shape[0]) if data[i,-1].item() == 0]).reshape(len([data[i:i+1,:] for i in range(data.shape[0]) if data[i,-1].item() == 0]),513))\n",
        "              N = np.random.randint(batch_size)\n",
        "              ix_pos = np.random.randint(pos_train.shape[0]-N)\n",
        "              ix_neg = np.random.randint(neg_train.shape[0]-batch_size-N)\n",
        "              pos_data = pos_train[ix_pos:ix_pos+N,:]\n",
        "              neg_data = neg_train[ix_neg:ix_neg+batch_size-N,:]\n",
        "              data = torch.concat((pos_data,neg_data))\n",
        "              seq = data[:,:-1]\n",
        "              targets = torch.zeros((batch_size,2), device=self.device)\n",
        "              o = -1\n",
        "              for i in data:\n",
        "                o+=1\n",
        "                if i[-1].item() == 1:\n",
        "                  targets[o][0] = 1\n",
        "                else:\n",
        "                  targets[o][1] = 1\n",
        "            else:\n",
        "              data = val_dataset\n",
        "              ix = np.random.randint(data.shape[0]-batch_size)\n",
        "              data = data[ix:ix+batch_size,:]\n",
        "              seq = data[:,:-1]\n",
        "              targets = torch.zeros((batch_size,2), device=self.device)\n",
        "              o = -1\n",
        "              for i in data:\n",
        "                o+=1\n",
        "                if i[-1].item() == 1:\n",
        "                  targets[o][0] = 1\n",
        "                else:\n",
        "                  targets[o][1] = 1\n",
        "            targets = targets.view(batch_size,2).to(self.device)\n",
        "            seq = seq.view(batch_size,512).to(self.device)\n",
        "            return seq, targets\n",
        "\n",
        "        @torch.no_grad\n",
        "        def cross_val():\n",
        "          model.eval()\n",
        "          out = []\n",
        "          for i in [\"train\",\"val\"]:\n",
        "            losses = torch.zeros(200+1)\n",
        "            for k in range(200):\n",
        "              X,Y = get_batch(i)\n",
        "              logits,loss = model(X,Y)\n",
        "              losses[k]=loss.item()\n",
        "              out1 = losses.mean()\n",
        "            out.append(out1)\n",
        "          model.train()\n",
        "          return out\n",
        "        losses = cross_val()\n",
        "        LOSS = [losses]\n",
        "        print(\"\\n[train loss = {k}, val loss =  {j}]\\n\".format(k = losses[0],j = losses[1]))\n",
        "        model.train()\n",
        "        for epoch in range(20):\n",
        "          print(\"[epoch {o}] \\n\".format(o = epoch))\n",
        "          iters = 200\n",
        "          for i in range(iters):\n",
        "            if i == 0:\n",
        "              Y = \"| =\"\n",
        "            elif i == iters -1 :\n",
        "              Y = \"=> 100% |\"\n",
        "            else:\n",
        "              if i%(int(iters/50)) == 0 :\n",
        "                Y = \"=\"\n",
        "              else:\n",
        "                Y = \"\"\n",
        "            print(\"{y}\".format(y = Y),end=\"\")\n",
        "            x, y = get_batch(\"train\")\n",
        "            logits, self.loss = model(x, y)\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            self.loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "            self.optimizer.step()\n",
        "          losses = cross_val()\n",
        "          LOSS.append(losses)\n",
        "          print(LOSS)\n",
        "          print(\"\\n[train loss = {k}, val loss =  {j}]\\n\".format(k = losses[0],j = losses[1]))\n",
        "        PATH = \"model {h}\".format(h = model_config.model_type)\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "class Transformer():\n",
        "  def __init__(self,mode):\n",
        "    self.mode = mode\n",
        "    self.device = torch.device(\"cuda\")\n",
        "    self.vocab_dict = tokenizer.get_vocab()\n",
        "\n",
        "  def classifier(self):\n",
        "    device = self.device\n",
        "    model_config = ClassifierI.get_default_config()\n",
        "    model_config = ClassifierI.get_default_config()\n",
        "    model_config.vocab_size = 25\n",
        "    model_config.block_size = 512\n",
        "    if self.mode == \"b\":\n",
        "      model_config.model_type = 'b'\n",
        "      model2 = ClassifierI(model_config)\n",
        "      model2.load_state_dict(torch.load(\"classi/model_b\",map_location=self.device))\n",
        "    else:\n",
        "      model_config.model_type = 'c'\n",
        "      model2 = ClassifierI(model_config)\n",
        "      model2.load_state_dict(torch.load(\"classi/model_c\",map_location = self.device))\n",
        "    model2.to(device)\n",
        "    return model2\n",
        "  def Encode(self,i):\n",
        "    def encode(x):\n",
        "      l = []\n",
        "      chars = self.vocab_dict\n",
        "      for i in x:\n",
        "        l.append(chars[i])\n",
        "      return l\n",
        "    def Padding(x,PAD = 0,max_len = 512):\n",
        "      l = []\n",
        "      max_len = 512\n",
        "      for i in x:\n",
        "        a = i\n",
        "        if len(a) < max_len:\n",
        "          a = a + [PAD for i in range(max_len-len(a))]\n",
        "        l.append(a)\n",
        "      return np.array(l).reshape(len(x),max_len)\n",
        "\n",
        "    return  torch.tensor(Padding(list(map(encode,i)))).view(len(i),512).to(self.device)\n",
        "\n",
        "  def Decode(self,i):\n",
        "    def decode(k):\n",
        "      l = [j for j in k if j != 0 ]\n",
        "      seq = [chars[j] for j in l]\n",
        "      return \"\".join(seq)\n",
        "    H = list(map(decode,i))\n",
        "    G = []\n",
        "    for i in H:\n",
        "      if len(i)>512:\n",
        "        u = H.index(i)\n",
        "        for j in range(len(i)-512):\n",
        "          G.append(i[j:j+512])\n",
        "      else:\n",
        "        G.append(i)\n",
        "    return G,u\n",
        "  def Mean(self,i):\n",
        "    I = list(i.ravel())\n",
        "    sig = lambda x:np.e**(0*x)\n",
        "    Sum = [sig(np.abs(x-int(len(I)/2))) for x in range(len(I))]\n",
        "    wei = np.array([Sum[i]/sum(Sum) for i in range(len(I))]).reshape(len(I),1)\n",
        "    return np.sum(wei*i)\n",
        "\n",
        "  def enhancer(self,x):\n",
        "    wei = np.array([np.exp((i[0]-1)/(i[0])) for i in x])\n",
        "    return np.sum(np.array([i[0] for i in x])*wei)/np.sum(wei)\n",
        "\n",
        "  def predict_proba(self,i):\n",
        "    if len(i[0])>512:\n",
        "       l = [self.Encode([i[0][j:j+512]]) for j in range(len(i[0])-512)]\n",
        "       if len(l)>700:\n",
        "        L = [torch.concat(tuple(l[i*700:(i+1)*700])) for i in range(len(l)//700)] + [torch.concat(tuple(l[(len(l)//700)*700:(len(l)//700)*700+len(l)%700]))]\n",
        "        T = [self.classifier().predict_proba(i,\"sig\").tolist() for i in L ]\n",
        "        return self.enhancer(np.concatenate(tuple([i for i in T])))\n",
        "       else:\n",
        "         t = torch.concat(tuple(l))\n",
        "         return self.enhancer(self.classifier().predict_proba(t,\"sig\").tolist())\n",
        "    else:\n",
        "       seq = self.Encode(i)\n",
        "       U = np.array(self.classifier().predict_proba(seq,\"sig\").tolist())\n",
        "       out = np.array(U)\n",
        "       return out.reshape(len(i),1)\n",
        "\n",
        "model2 = Transformer(\"b\")\n",
        "model3 = Transformer(\"c\")\n",
        "\n",
        "\n",
        "import functools as FUNC\n",
        "from numba import njit\n",
        "class PPA():\n",
        "    def __init__(self, seq, model):\n",
        "        self.model = model\n",
        "        self.seq = seq\n",
        "        self.s = None\n",
        "        self.len = len(self.seq)\n",
        "        self.idx = None\n",
        "        self.p = None\n",
        "        self.dir = None\n",
        "        self.name = None\n",
        "\n",
        "    def attention_mask(self):\n",
        "        idx2 = self.Splicer(self.seq)[1]\n",
        "        k = self.T()\n",
        "        mask = []\n",
        "        for j in range(len(k)):\n",
        "           mask.append([True if len(k[0])>idx2[j]-i>-1 else False for i in range(len(k[j]))])\n",
        "        return mask\n",
        "\n",
        "    def Mean(self, mask, spliced):\n",
        "       u = []\n",
        "       for i in trange(len(spliced)):\n",
        "          o = self.model.predict_proba([spliced[i][j] for j in range(len(spliced[i])) if mask[i][j] == True])\n",
        "          u.append(np.mean(o))\n",
        "       return u\n",
        "\n",
        "    def SPLICE_scorer(self, i):\n",
        "        o = []\n",
        "        lmin = int(np.floor(len(i)/2))\n",
        "        lmax = int(np.ceil(len(i)/2))\n",
        "        for j in range(len(i) - lmin+1):\n",
        "            o.append(i[j:j + lmin])\n",
        "        return o\n",
        "\n",
        "    def Splicer(self, i):\n",
        "        o = []\n",
        "        Before = []\n",
        "        idx = self.idx\n",
        "        for j in range(len(i)):\n",
        "            n_b = j\n",
        "            n_a = len(i) - j - 1\n",
        "            if n_b <= int(np.floor(idx/2)):\n",
        "                before = i[j - min(int(np.floor(idx/2)), n_b):j]\n",
        "                after = i[j + 1:j + idx - len(before)]\n",
        "            elif n_a <= int(np.ceil(idx/2)):\n",
        "                before = i[j - idx + n_a+1:j]\n",
        "                after = i[j + 1:]\n",
        "            else:\n",
        "                before = i[j - min(int(np.floor(idx/2)), n_b):j]\n",
        "                after = i[j + 1:j + min(int(np.ceil(idx/2)), n_a)]\n",
        "            o.append(before + i[j] + after)\n",
        "            Before.append(len(before))\n",
        "        return o,Before\n",
        "\n",
        "    def T(self):\n",
        "        return list(map(self.SPLICE_scorer, self.Splicer(self.seq)[0]))\n",
        "\n",
        "    def Out(self):\n",
        "        def d(x):\n",
        "            if self.s>0.7:\n",
        "                return x*np.exp(-1.2*(x-self.s))\n",
        "            else:\n",
        "                if x > 0.7 :\n",
        "                    return x\n",
        "                else:\n",
        "                    #x * np.exp(-2 * np.abs(x - self.s))\n",
        "                    return x\n",
        "        return list(map(d,self.Mean(self.attention_mask(),self.T())))\n",
        "\n",
        "\n",
        "\n",
        "    def show(self,name:str):\n",
        "        import matplotlib.pyplot as plt\n",
        "        i = self.p\n",
        "        values = np.array(i).reshape(1, len(i))\n",
        "        y = range(len(i))\n",
        "        Y = [(j,1) for j in range(len(i)) if np.mean([i[j],i[min((j+1),len(i)-1)],i[max(0,(j-1))]])>0.6]\n",
        "        Y2 = [(j,1) for j in range(len(i)) if np.mean([i[j],i[min((j+1),len(i)-1)],i[min((j+2),len(i)-1)],i[min((j+3),len(i)-1)],i[min((j+4),len(i)-1)],i[min((j+5),len(i)-1)],i[max(0,(j-1))],i[max(0,(j-2))],i[max(0,(j-3))],i[max(0,(j-4))],i[max(0,(j-5))]])>0.85]\n",
        "        threshold = 0.5\n",
        "        above_threshold = np.maximum(values - threshold, 0)\n",
        "        below_threshold = np.minimum(values, threshold)\n",
        "        fig, (ax3 , ax4) = plt.subplots(2,1,sharex = True,sharey = True)\n",
        "        ax3.bar(y, below_threshold.ravel(), 0.9, color=\"lightblue\")\n",
        "        ax3.bar(y, above_threshold.ravel(), 0.9, color=\"lightgreen\",\n",
        "               bottom=below_threshold.ravel())\n",
        "        ax4.broken_barh(Y, (0.7, 0.03),\n",
        "               facecolors=( 'red'))\n",
        "        ax4.plot([0., y[-1]],[0.715,0.715],\"-b\")\n",
        "        for k in Y:\n",
        "            ax4.plot([k[0],k[0]+1],[0.715,0.715],\"-r\")\n",
        "\n",
        "        ax4.text(0,0.8 , \"LLPS-susceptible regions\",fontsize = 20)\n",
        "        ax3.plot([0., y[-1]], [threshold, threshold], \"k--\")\n",
        "        ax4.set_ylim([0,1])\n",
        "        ax3.set_ylim([0,1])\n",
        "\n",
        "        plt.subplots_adjust(\n",
        "                    wspace=0.4,\n",
        "                    hspace=0)\n",
        "        fig.set_figwidth(20)\n",
        "        fig.set_figheight(15)\n",
        "        plt.show()\n",
        "        if self.dir != None:\n",
        "          if self.name != None:\n",
        "            fig.savefig(self.dir+\"\\\\\"+self.name)\n",
        "          else:\n",
        "            name = np.random.randint(1000000)\n",
        "            fig.savefig(self.dir+\"\\\\\"+\"figure,sequenceID:\"+str(name)+\".png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0165aad0cd5140379fdcbf71a3910b40",
            "514f6e75fab641e8a1ee8c17570dea2f",
            "4acb7b436f8645afb23eee8dbfe83215",
            "4b2fc4d296e94d8eb9bc1c9ab79e035a",
            "d4374580d1b24785b9361b8e7b464388",
            "3238df640f4a4d8e9fb6f1dba8937c09",
            "fd1f0dc91db34c54a402b437b54d962d",
            "1071d55dd7b542e9a9f90e8fdcda9dce",
            "b613365935f4424ca1598f8df949bb78",
            "563cf88e91ef4ae9a9e91534387248dd",
            "654f2bb221da4833a0551940a92ca62e"
          ]
        },
        "id": "_R4AYlL7PfO2",
        "outputId": "b1a7f8a4-6fcd-4f37-cd74-a6d7acd37342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The request to the Slack API failed. (url: https://www.slack.com/api/chat.postMessage)\n",
            "The server responded with: {'ok': False, 'error': 'not_authed'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/905 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0165aad0cd5140379fdcbf71a3910b40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score : 0.6237879418238705\n"
          ]
        }
      ],
      "source": [
        "#@title Loading sequence , then hit `Runtime` -> `Run all`\n",
        "Sequence = \"MSGRGKQGGKARAKAKTRSSRAGLQFPVGRVHRLLRKGNYSERVGAGAPVYLAAVLEYLTAEILELAGNAARDNKKTRIIPRHLQLAIRNDEELNKLLGRVTIAQGGVLPNIQAVLLPKKTESHHKAKGKYKAAVDLSHFLKEKGGLEGLIHSQRRQDILDLWIYHTQGYFPDWQNYTPGPGIRYPLTFGWCYKLVPVEPDKVEEANKGENTSLLHPVSLHGMDDPEREVLEWRFDSRLAFHHVARELHPEYFKNCYTSLNHSLIEEIQWQQEKNFQELLEKYKWKSLWRWWASEPNQSLPVNDPRSEQKPPKSESKGIVQQQNDLLDAIEAQQHLLQLTVWGIKQLQARVLPPPPPPPPPPPPPPQQPPPPPPPPPPPQQQQQQQQQQPPPPWPWPWPWPWPWPQPQPWPQPWPQPWPSPSPSPSPPPPPPPPPENFKHLPEPFRIRVIEPVKRTTRAYREEAIIKSGMNPFLLDSEDVFIDLLTDSGTGAVTQSMQAAMMRGDEAYSGSRSYYALAESVKNIFGYQYTIPTHQGRGAEQIYIPVLIKKREQEKGLDRSKMVAFSNYFFDTTQGHSQINGCTVRNVYIKEAFDTGVRYDFKGNFDLEGLERGIEEVGPNNVPYIVATITSNSAGGQPVSLANLKAMYSIAKKYDIPVVMDSARFAENAYFIKQREAEYKDWTIEQITRETYKYADMLAMSAKKDAMVPMGGLLCMKDDSFFDVYTECRTLCVVQEGFPTYGGLEGGAMERLAVGLYDGMNLDWLAYRIAQVQYLVDGLEEIGVVCQQAGGHAAFVDAGKLLPHIPADQFPAQALACELYKVAGIRAVEIGSFLLGRDPKTGKQLPCPAELLRLTIPRATYTQTHMDFIIEAFKHVKENAANIKGLTFTYEPKVLRHFTAKLKEV\" #@param {type:\"string\"}\n",
        "name = None #@param {type:\"string\"}\n",
        "directory = None #@param {type:\"string\"}\n",
        "def enh(x):\n",
        "  wei = np.array([np.exp((i-1)/(i)) for i in x])\n",
        "  return np.sum(np.array(x)*wei)/np.sum(wei)\n",
        "\n",
        "def edit(i):\n",
        "  for j in i:\n",
        "    if j in \"\\t\\n \":\n",
        "      i = i.replace(j,\"\")\n",
        "  return i\n",
        "Sequence = edit(Sequence)\n",
        "\n",
        "u = model3.predict_proba([Sequence])\n",
        "\n",
        "c = PPA(Sequence,model3)\n",
        "c.s = np.mean(u)\n",
        "c.idx = int(np.ceil(0.1*len(Sequence)))\n",
        "p = c.Out()\n",
        "c.p = p\n",
        "c.dir = directory\n",
        "c.name = name\n",
        "c.show(\"l\")\n",
        "print(\"Score : \"+str((2*u+2*enh(p))/4))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOzG3PSL11x6NF7Vdibmldv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0165aad0cd5140379fdcbf71a3910b40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_514f6e75fab641e8a1ee8c17570dea2f",
              "IPY_MODEL_4acb7b436f8645afb23eee8dbfe83215",
              "IPY_MODEL_4b2fc4d296e94d8eb9bc1c9ab79e035a"
            ],
            "layout": "IPY_MODEL_d4374580d1b24785b9361b8e7b464388"
          }
        },
        "514f6e75fab641e8a1ee8c17570dea2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3238df640f4a4d8e9fb6f1dba8937c09",
            "placeholder": "​",
            "style": "IPY_MODEL_fd1f0dc91db34c54a402b437b54d962d",
            "value": "100%"
          }
        },
        "4acb7b436f8645afb23eee8dbfe83215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1071d55dd7b542e9a9f90e8fdcda9dce",
            "max": 905,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b613365935f4424ca1598f8df949bb78",
            "value": 905
          }
        },
        "4b2fc4d296e94d8eb9bc1c9ab79e035a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_563cf88e91ef4ae9a9e91534387248dd",
            "placeholder": "​",
            "style": "IPY_MODEL_654f2bb221da4833a0551940a92ca62e",
            "value": " 905/905 [03:21&lt;00:00,  6.01it/s]"
          }
        },
        "d4374580d1b24785b9361b8e7b464388": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3238df640f4a4d8e9fb6f1dba8937c09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd1f0dc91db34c54a402b437b54d962d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1071d55dd7b542e9a9f90e8fdcda9dce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b613365935f4424ca1598f8df949bb78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "563cf88e91ef4ae9a9e91534387248dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "654f2bb221da4833a0551940a92ca62e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}