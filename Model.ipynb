{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CJ256BaIQqz"
      },
      "outputs": [],
      "source": [
        "#block_size = 256\n",
        "class SiLU(nn.Module):\n",
        "   def forward(self, x):\n",
        "        return x*F.sigmoid(x)\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class SA(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.key = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.C= nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.dropout2 = nn.Dropout(config.resid_pdrop)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        att = (q @ k.transpose(-2, -1)) / (k.size(-1))**0.5\n",
        "        for i in range(block_size):\n",
        "          for j in range(i + 1):\n",
        "            att[i, j] = 1.0\n",
        "        self.register_buffer(\"bias\", att.view(1, 1, block_size, block_size))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.dropout2(self.C(y))\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.LN1 = nn.LayerNorm(config.n_embd)\n",
        "        self.LN2 = nn.LayerNorm(config.n_embd)\n",
        "        self.attention = SA(config)\n",
        "        self.FeedForward = nn.ModuleDict(dict(\n",
        "            l1    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            l2  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            l3    = GELU(),\n",
        "            dropout = nn.Dropout(config.resid_pdrop),\n",
        "        ))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x+self.attention(self.LN1(x))\n",
        "        x = x+self.FeedForward(self.LN2(x))\n",
        "        return x\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.Re = Reinforcer()\n",
        "        self.block_size = config.block_size\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.embd_pdrop),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        device = \"cpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        config = cls.get_default_config()\n",
        "        config.model_type = model_type\n",
        "        config.vocab_size = 20\n",
        "        config.block_size = 256\n",
        "        model = Model(config)\n",
        "        sd = model.state_dict()\n",
        "\n",
        "\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')]\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn\n",
        "                if pn.endswith('bias'):\n",
        "\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0,% (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
        "        tokenizer =\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(idx)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k != None:\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                for i in range(logits.shape[0]):\n",
        "                  for j in range(logits.shape[1]):\n",
        "                    if logits[i, j] < v[i, -1]:\n",
        "                      logits[i, j] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            if do_sample:\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "                Score = []\n",
        "                P = []\n",
        "                for i in range(self.vocab_size):\n",
        "                  for j in range(probs.size()[0]):\n",
        "                    if j == i:\n",
        "                      P.append(probs[0,j])\n",
        "                    else:\n",
        "                      P.append(0)\n",
        "                  Score.append(self.Re.search(tokenizer.decode(torch.cat((idx,torch.tensor(P).view(len(self.vocab_size,1))),dim = 1))).tolist()[0])\n",
        "                idx_next = torch.cat( (idx,torch.multinomial(P[np.array(Score).argmax()]),dim = 1) )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            if idx[-1][-1].item() == tokenizer.encode([[\"#\"]]):\n",
        "              idx_next = tokenizer.encode([[\"M\"]])\n",
        "            idx = torch.cat((idx,idx_next),dim = 1)\n",
        "\n",
        "        return idx"
      ]
    }
  ]
}