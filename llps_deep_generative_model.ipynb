{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Required Libraries and Packages"
      ],
      "metadata": {
        "id": "sl1uMm1GHUV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install virtualenv\n",
        "!virtualenv venv\n",
        "!source venv/bin/activate"
      ],
      "metadata": {
        "id": "L4SqVT65Hi_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt-get install chromium chromium-driver\n",
        "!apt-get install -y chromium-browser\n",
        "!apt install -y chromium-chromedriver\n",
        "!ls /usr/lib/chromium-browser/chromedriver\n",
        "!pip install webdriver-manager\n",
        "\n",
        "from selenium import webdriver\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "from selenium.webdriver.support.wait import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import selenium\n"
      ],
      "metadata": {
        "id": "xRvxq8l7Gb-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 256\n",
        "class SiLU(nn.Module):\n",
        "   def forward(self, x):\n",
        "        return x*F.sigmoid(x)\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class SA(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.key = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.C= nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.dropout2 = nn.Dropout(config.resid_pdrop)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        att = (q @ k.transpose(-2, -1)) / (k.size(-1))**0.5\n",
        "        for i in range(block_size):\n",
        "          for j in range(i + 1):\n",
        "          att[i, j] = 1.0\n",
        "        self.register_buffer(\"bias\", att.view(1, 1, block_size, block_size))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.dropout2(self.C(y))\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.LN1 = nn.LayerNorm(config.n_embd)\n",
        "        self.LN2 = nn.LayerNorm(config.n_embd)\n",
        "        self.attention = SA(config)\n",
        "        self.FeedForward = nn.ModuleDict(dict(\n",
        "            l1    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            l2  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            l3    = GELU(),\n",
        "            dropout = nn.Dropout(config.resid_pdrop),\n",
        "        ))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x+self.attention(self.LN1(x))\n",
        "        x = x+self.FeedForward(self.LN2(x))\n",
        "        return x\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.Re = Reinforcer()\n",
        "        self.block_size = config.block_size\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.embd_pdrop),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        device = \"cpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        config = cls.get_default_config()\n",
        "        config.model_type = model_type\n",
        "        config.vocab_size = 20\n",
        "        config.block_size = 256\n",
        "        model = Model(config)\n",
        "        sd = model.state_dict()\n",
        "\n",
        "\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')]\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn\n",
        "                if pn.endswith('bias'):\n",
        "\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0,% (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
        "        tokenizer =\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(idx)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k != None:\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                for i in range(logits.shape[0]):\n",
        "                  for j in range(logits.shape[1]):\n",
        "                    if logits[i, j] < v[i, -1]:\n",
        "                      logits[i, j] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            if do_sample:\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "                Score = []\n",
        "                P = []\n",
        "                for i in range(self.vocab_size):\n",
        "                  for j in range(probs.size()[0]):\n",
        "                    if j == i:\n",
        "                      P.append(probs[0,j])\n",
        "                    else:\n",
        "                      P.append(0)\n",
        "                  Score.append(self.Re.search(tokenizer.decode(torch.cat((idx,torch.tensor(P).view(len(self.vocab_size,1))),dim = 1))).tolist()[0])\n",
        "                idx_next = torch.cat( (idx,torch.multinomial(P[np.array(Score).argmax()]),dim = 1) )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            if idx[-1][-1].item() == tokenizer.encode([[\"#\"]]):\n",
        "              idx_next = tokenizer.encode([[\"M\"]])\n",
        "            idx = torch.cat((idx,idx_next),dim = 1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "jxY5hiFAYcQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(self, config, model, train_dataset):\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.optimizer = None\n",
        "        self.train_dataset = train_dataset\n",
        "\n",
        "        self.device = 'cpu'\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.iter_num = 0\n",
        "        self.iter_time = 0.0\n",
        "        self.iter_dt = 0.0\n",
        "\n",
        "    def add_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent].append(callback)\n",
        "\n",
        "    def set_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent] = [callback]\n",
        "\n",
        "    def trigger_callbacks(self, onevent: str):\n",
        "        for callback in self.callbacks.get(onevent, []):\n",
        "            callback(self)\n",
        "\n",
        "    def run(self):\n",
        "        model, config = self.model, self.config\n",
        "        device = \"cpu\"\n",
        "        self.optimizer = model.configure_optimizers(config)\n",
        "\n",
        "        def get_batch():\n",
        "          data = self.train_dataset\n",
        "          block_size = 100\n",
        "          ix = torch.randint(data.shape[0]-block_size,(config.batch_size,))\n",
        "          x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "          y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "          x,y = x.to(device),y.to(device)\n",
        "          return x,y\n",
        "\n",
        "        @torch.no_grad\n",
        "        def cross_val():\n",
        "          model.eval()\n",
        "          losses = torch.zeros(200+1)\n",
        "          for k in range(200):\n",
        "            X,Y = get_batch()\n",
        "            logits,loss = model(X,Y)\n",
        "            losses[k]=loss.item()\n",
        "            out = losses.mean()\n",
        "          model.train()\n",
        "          return out\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(15):\n",
        "          print(\"[epoch {o}] \\n\".format(o = epoch))\n",
        "\n",
        "          for i in range(50):\n",
        "            x, y = get_batch()\n",
        "            logits, self.loss = model(x, y)\n",
        "\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            self.loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "            self.optimizer.step()\n",
        "          print(\"\\n[train loss = {k} ]\\n\".format(epoch = epoch , k = cross_val()))\n",
        "\n"
      ],
      "metadata": {
        "id": "-kWEJdJCYpFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import selenium\n",
        "from selenium import webdriver\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "\n",
        "\n",
        "class Reinforcer():\n",
        "\n",
        "  def web_driver(self):\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(\"--verbose\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument(\"--disable-gpu\")\n",
        "    options.add_argument(\"--window-size=1920, 1200\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    driver = webdriver.Chrome(options = options)\n",
        "    return driver\n",
        "\n",
        "  def Parm(self,inforcer = \"Fuzdrop\"):\n",
        "\n",
        "\n",
        "     if inforcer == \"Fuzdrop\":\n",
        "      id = \"floatingTextarea2\"\n",
        "      class_name = \"form-control ng-pristine ng-valid ng-touched\"\n",
        "      place_holder = \"Leave a comment here\"\n",
        "      style = \"height: 10rem;\"\n",
        "      url = \"https://fuzdrop.bio.unipd.it/predictor\"\n",
        "     else:\n",
        "      id = \"inputseq\"\n",
        "      class_name = None\n",
        "      place_holder = \"> Seq 1MASNDYTQQATQSYGAYPTQPGQGYSQQSSQPYGQQSYSGYSQSTDTSGYGQSSYSSYGQSQNTGYGTQSTPQGYGSTGGYGSSQSSQSSYGQQSSYPGYGQQPAPSSTSGSYGSSSQSSSYGQPQSGSYSQQPSYGGQQQSYGQQQSYNPPQGYGQQNQYNSSSGGGGGGGGGGNYGQDQSSMSSGGGSGGGYGNQDQSGGGGSGGYGQQDRGGRGRGGSGGGGGGGGGGYNRSSGGYEPRGRGGGRGGRGGMGGSDRGGFNKFGGPRDQGSRHDSEQDNSDNNTIFVQGLGENVTIESVADYFKQIGIIKTNKKTGQPMINLYTDRETGKLKGEATVSFDDPPSAKAAIDWFDGKEFSGNPIKVSFATRRADFNRGGGNGRGGRGRGGPMGRGGYGGGGSGGGGRGGFPSGGGGGGGQQRAGDWKCPNPTCENMNFSWRNECNQCKAPKPDGPGGGPGGSHMGGNYGDDRRGGRGGYDRGGYRGRGGDRGGFRGGRGGGDRGGFGPGKMDSRGEHRQDRRERPY\"\n",
        "      style = None\n",
        "      url = \"http://www.pkumdl.cn:8000/PSPredictor/\"\n",
        "\n",
        "      d = {\"id\" : id,\n",
        "      \"class_name\" : class_name,\n",
        "      \"place_holder\" : place_holder,\n",
        "      \"style\" : style,\n",
        "      \"url\":url }\n",
        "\n",
        "     return d\n",
        "\n",
        "\n",
        "  def By(self):\n",
        "\n",
        "    if self.param == \"class_name\":\n",
        "      d = By.CLASS\n",
        "    if self.param == \"id\":\n",
        "      d = By.ID\n",
        "    if self.param == \"style\":\n",
        "      d = By.style\n",
        "    return d\n",
        "\n",
        "\n",
        "  def search(self,seq,inforcer,param):\n",
        "\n",
        "    assert param in [\"class_name\" , \"id\" , \"placeholder\",\"formcontrolname\",\"style\"]\n",
        "    assert inforcer in [\"pspredictor\",\"Fuzdrop\"]\n",
        "    parameters = self.Parm(self.inforcer)\n",
        "    id,class_name ,placeholder,formcontrolname,style,url = parameters[\"id\"],parameters[\"class_name\"],parameters[\"placeholder\"],parameters[\"formcontrolname\"],parameters[\"style\"],parameters[\"url\"]\n",
        "    searching_param = parameters[self.param]\n",
        "    driver = self.web_driver()\n",
        "    search = driver.find_element(self.By(),searching_param)\n",
        "    search.send_keys(seq)\n",
        "    button = driver.find_element(By.ID,\"btn_predict\")\n",
        "    button.click()\n",
        "\n",
        "    try:\n",
        "      main = WebDriverWait(driver, 20).until(EC.presence_of_element_located((self.By(),searching_param)))\n",
        "    except:\n",
        "        driver.quit()\n",
        "    score = float(main.text[12+len(self.seq):-10])\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PITKMy3R0tNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edit(x):\n",
        "    P = []\n",
        "    for i in x:\n",
        "      if i in \"BJOUX\\Z_\":\n",
        "        x = x.replace(i,\"\")\n",
        "      else:\n",
        "        pass\n",
        "    for i in x:\n",
        "      if i == \"n\":\n",
        "        x = x.replace(i,\"#\")\n",
        "      else:\n",
        "        pass\n",
        "    for i in range(len(x)-1):\n",
        "      if x[i] == \"#\" and x[i+1] == \"#\":\n",
        "        pass\n",
        "      else:\n",
        "        P.append(x[i])\n",
        "\n",
        "    return \"\".join(P)\n",
        "\n",
        "\n",
        "with open(\"training.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "  v = text.split(\"\\\\n<|endoftext|>\\\\n\")\n",
        "  v = \"\".join(v)\n",
        "  V = edit(v)\n",
        "  V = V[14:]\n",
        "chars = sorted(list(set(V)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "with open(\"validation.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "  o = text.split(\"<|endoftext|>\")\n",
        "  o = \"\".join(o)\n",
        "  O = edit(o)\n",
        "  O = O[14:]\n",
        "\n",
        "def encode(x):\n",
        "  l=[]\n",
        "  for i in x:\n",
        "    l.append(chars.index(i))\n",
        "  return l\n",
        "\n",
        "def decode(x):\n",
        "  l = []\n",
        "  for i in x:\n",
        "    l.append(chars[i])\n",
        "  return \"\".join(l)\n",
        "\n",
        "\n",
        "train_data = torch.tensor(encode(V),dtype = torch.long)\n",
        "val_data = torch.tensor(encode(O),dtype = torch.long)"
      ],
      "metadata": {
        "id": "tRVRuzisgUwx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}